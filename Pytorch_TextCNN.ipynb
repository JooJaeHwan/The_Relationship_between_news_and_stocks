{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def str_to_list(d):\n",
    "  text = re.sub(r'[\\[\\'\\]]', '', d)\n",
    "  return text.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy.data import Field\n",
    "# text 필드\n",
    "TEXT = Field(tokenize = str_to_list) # 내가 만든 토크나이즈\n",
    "\n",
    "# label 필드\n",
    "LABEL = Field(sequential = False) # label은 문장 단어 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['김', '다나', '디자인', '기자', '남', '이야기', '를', '듣', '지', '않', '고', '라', '떼', '만', '계속', '이야기', '하', '는', '기업', '은', '꼰대', '로', '낙인찍히', '게', '되', 'ㅂ니다', '최', '태원', 'SK', '그룹', '회장', '의', '신', '기업가', '정신', '이', 'MZ', '세대', '직장인', '들', '의', '마음', '을', '홀리', '었', '다', 'SK', '그룹', '을', '재계', '서열', '2', '위', '로', '올려놓', '은', '양적', '성장', '이외', '에', '도', '수평적', '소통', '과', '사내', '복지', '에', '힘쓰', '는', '새', '경영', '철학', '을', '내세우', '면서', '다', '업계', '는', '창업', '2', '세대', '로', '꼽히', '는', '60', '대', '재벌', '총수', '의', '수평적', '화법', '이', '긍정적', '이', 'ㄴ', '평가', '를', '받', '고', '있', '다고', '분석', '하', 'ㄴ다', '10', '일', '업계', '에', '따르', '면', '최', '회장', '은', '최근', '꼰대', '에서', '벗', '어', '나', '기업', '의', '새', '역할', '을', '강조', '하', '는', '신', '기업가', '정신', '을', '경영', '화두', '로', '내세우', '었', '다', '이윤', '창출', '을', '넘', '어', '사회적', '공헌', '에', '기여', '하', '고', '기후', '변화', '환경', '오염', '등', '여러', '방면', '의', '문제', '를', '기업', '이', '적극', '해결', '하', '어야', '하', 'ㄴ다는', '주장', '이', '다', '최', '회장', '은', '한국', '사회', '에', '반', '기업', '정서', '가', '팽배', '하', '어', '있', '다며', '기업', '신뢰', '제고', '를', '위하', '어', '선제', '적', '실천', '을', '강조', '하', '었', '다', 'MZ', '세대', '가', '관심', '을', '갖', '는', '사내', '소통', '과', '재택', '근무', '등', '복지', '도', '주의', '를', '기울이', 'ㄴ다', '1960', '년생', '으로', '올해', '62', '세', '이', 'ㄴ', '최', '회장', '은', '지나', 'ㄴ', '3', '월', 'SK', '텔레콤', '인공지능', '사업', '팀원', '들', '과', '만', '나', '임', '직원', '들', '과', '격의', '없', '는', '소통', '을', '하', '겠', '다', '며', '앞', '으로', '토', '니', '로', '불르', '어', '닿', 'ㄹ라', '이', '고', '하', '었', '다', '지나', 'ㄴ', '4', '월', '에', '도', '재택', '근무', '로', '생활', '패턴', '에', '변화', '가', '생기', 'ㄴ', '워킹', '맘', '을', '예', '로', '들', '며', '본인', '도', '1', '달', '넘', '게', '재택근무', '를', '하', '면서', '많', '은', '점', '을', '느끼', '고', '있', '다', '이', '고', '언급', '하', '었', '다', 'SK', '그룹', '의', '핵심', '계열', '사인', 'SK', '하이닉스', '를', '중심', '으로', 'ESG', '행', '보도', '강화', '하', '고', '있', '다', 'SK', '하이닉스', '는', '지난해', '그룹', '전체', '사회적', '가치', '성적', '이', 'ㄴ', '18', '조', '4000', '억', '원', '의', '절반', '이상인', '9', '조', '4173', '억', '원', '의', '사회적', '가치', '를', '창출', '하', '었', '다', '여기', '에', 'SK', '그룹', '은', '오', '는', '2026', '년', '까지', '247', '조', '원', '을', '투자', '하', '고', '이중', '약', '30', '에', '달', '하', '는', '67', '조', '4000', '억', '원', '을', '전기', '차', '배터리', '설비', '와', '수소', '풍력', '등', '그린', '비즈니스', '에', '투입', '하', 'ㄴ다', '최', '회장', '이', '잇달', 'ㄴ', '파격', '행보', '에', '나서', '는', '데', '에', '는', '최근', '기업', '의', '사회적', '책임', '을', '강조', '하', '는', 'MZ', '직장인', '소비자', '가', '경제', '의', '주요', '축', '으로', '부상', '하', 'ㄴ', '것', '에', '대하', 'ㄴ', '인식', '이', '깔리', '어', '있다', '지나', 'ㄴ', '4', '월', '대한', '상의', '가', 'MZ', '세대', '380', '명', '을', '대상', '으로', 'ESG', '경영', '과', '기업', '의', '역할', '을', '조사', '하', 'ㄴ', '결과', '기업', '의', '바람직', '하', 'ㄴ', '역할', '로', '투명', '윤리', '경영', '실천', '오', '아', '환경', '보호', '를', '꼽', '았', '다', '또', '10', '명', '중', '6', '명', '은', 'ESG', '경영', '을', '실천', '하', '는', '기업', '제품', '을', '구매', '하', '겠', '다고', '응답', '하', '었', '다', '재계', '는', '젊', '은', '직장인', '들', '의', '이직', '과', '퇴사', '가', '잦아지', '는', '시기', '기업', '총수', '의', '수평적', '소통', '행보', '가', '크', 'ㄴ', '호응', '을', '얻', '을', '것', '이', '라고', '평가', '하', 'ㄴ다', '재계', '관계자', '는', '나이', '와', '직위', '를', '내려놓', '고', '적극', '소통', '하', '는', '모습', '이', 'MZ', '세대', '직장', '인들', '로', '부터', '크', 'ㄴ', '호응', '을', '얻', '고', '있다', '면', '서', '보수', '뿐', '만', '아니', '라', '재택근무', '와', '유연', '하', 'ㄴ', '사내', '문화', '가', '직장', '을', '선택', '하', '는', '주요', '기준', '으로', '부상', '하', 'ㄴ', '시기', '에', '신', '기업가', '정신', '이', '직원', '들', '의', '사기', '를', '증진', '하', 'ㄹ', '수', '있', '다', '이', '고', '분석', '하', '었', '다'] 0\n",
      "['2500', '억', '모집', '액', '못', '채우', '고', '미달', '3', '년', '물', '2200', '억', '에', '1710', '억', '모', '집', '5', '년', '물', '전액', '미달', '계열사', '실적', '변동성', '크', '고', '후', '순', '위성', '그룹', '전반', '부동산', 'PF', '익스', '포저', '과중', '메리', '츠', '금융', '지주', '공모채', '수요', '예측', '에서', '미달', '이', '발생', '하', '었', '다', '대부분', '금융', '계열사', '이', '다', '보', '니', '실적', '변동성', '이', '높', '고', '그룹', '전반', '의', '부동산', '프로젝트', '파이', '내', 'ㄴ', '싱', '익스포저', '가', '과중', '하', '어', '기관', '투자가', '자금', '유인', '에', '실패', '하', '었', '다는', '분석', '이', '다', '15', '일', '금융', '투자', '업계', '에', '따르', '면', '메리', '츠', '금융', '지주', '가', '이날', '진행', '하', 'ㄴ', '무', '기명식', '이권', '부', '무보증', '사채', '수요', '예측', '에서', '모집', '액', '2500', '억', '원', '을', '채우', '지', '못하', '고', '일부', '미달', '이', '발생', '하', 'ㄴ', '것', '으로', '전하', '어', '지', 'ㄴ다', '메리', '츠', '금융', '지주', '는', '수요', '예측', '결과', '에', '따르', '아', '최대', '3500', '억', '원', '까지', '증액', '발행', '도', '검토', '하', '었', '었', '다', '메리', '츠', '금융', '지주', '는', '이번', '수요', '예측', '에서', '만기', '구조', '를', '3', '년', '과', '5', '년', '물', '로', '짜', '었', '고', '모집', '액', '은', '각각', '2200', '억', '원', '300', '억', '원', '으로', '구성', '하', '었', '다', '수요', '예측', '에서', '3', '년', '물', '에', '는', '1710', '억', '원', '만', '모집', '되', '었', '고', '5', '년', '물', '은', '전액', '미달', '이', 'ㄴ', '것', '으로', '전하', '어', '지', 'ㄴ다', '이번', '발행', '대표', '주관', '은', 'NH', '투자', '증권', '과', 'KB', '증권', '한국', '투자', '증권', '이', '맡', '았', '고', '신한', '금융', '투자', '교보', '증권', '신영', '증권', '등', '이', '인수단', '에', '참여', '하', '었', '다', '특히', '최근', '회사채', '발행', '시장', '의', '투자', '심리', '를', '고려', '하', '어', '3', '년', '물', '과', '5', '년', '물', '금리', '밴드', '도', '다르', '게', '짜', '었', '다', '3', '년', '물', '은', '메리', '츠', '금융', '지주', '3', '년', '회사채', '개별', '민', '평', '수익률', '의', '산술', '평균', '에', '30', 'bp', '30', 'bp', '를', '5', '년', '물', '에', '는', '40', 'bp', '40', 'bp', '를', '가산', '하', 'ㄴ', '이자율', '을', '제시', '하', '었', '다', '지나', 'ㄴ', '14', '일', '기준', '메리', '츠', '금융', '지주', '회사채', '3', '년', '물', '개별', '민', '평', '은', '4', '246', '이', '며', '5', '년', '물', '은', '4', '277', '다', '한', '증권사', 'DCM', '담당자', '는', '최근', '투자', '심리', '가', '악화', '되', '었', '다고', '하', '더라도', 'AA', '급', '에서', '미달', '이', '발생', '하', 'ㄹ', '수준', '은', '아니다', '며', '은행', '과', '제조업', '지주사', '대비', '메리', '츠', '금융', '지주', '의', '실적', '변동성', '이', '크', '다', '는', '점', '을', '기관', '투자가', '들', '이', '고려', '하', 'ㄴ', '것', '으로', '보이', 'ㄴ다', '또', '메리', '츠', '금융', '지주', '는', '구조적', '으로', '후', '순', '위성', '을', '띄', 'ㄴ다', '고', '설명', '하', '었', '다', '메리', '츠', '금융', '지주', '는', '메리츠화재', '와', '메리', '츠', '증권', '등', '을', '자회사', '로', '메리츠캐피탈', '을', '손자', '회사', '로', '보유', '하', '고', '있', '다', '신용', '등급', '에', '는', '주력', '자회사', '의', '신용도', '와', '지주', '회사', '로서', '주력', '자회사', '에', '대하', 'ㄴ', '구조적', '후', '순', '위성', '을', '반영', '하', '고', '있', '다', '실제', '한국', '신용', '평가', '는', '메리', '츠', '금융', '지주', '에', '대하', '어', '메리', '츠', '금융', '지주', '재무', '부담', '이', '재무', '지표', '상', '드러나', 'ㄴ', '것', '보다', '높', '다고', '평가', '하', '었', '다', '김', '선영', '한신', '평', '연구원', '은', '주요', '자회사', '증자', '와', '지급', '보증', '제공', '으로', '메리', '츠', '금융', '지주', '재무', '부담', '이', '높', '은', '편', '이', '다', '며', '설립', '이후', '자회사', '지분', '투자', '를', '지속', '하', '고', '있', '다', '향후', '에', '도', '규제', '변화', '등', '의', '이유', '로', '주력', '자회사', '의', '자본', '확충', '필요성', '은', '증대', '되', 'ㄹ', '것', '으로', '보이', 'ㄴ다', '이', '고', '설명', '하', '었', '다', '메리', '츠', '증권', '상환', '전환', '우선주', '에', '대하', 'ㄴ', '총수익', '스왑', '3400', '억', '원', '과', '메리츠캐피탈', '이', '발행', '하', 'ㄴ', '회사채', '와', '기업', '어음', '에', '대하', 'ㄴ', '메리', '츠', '금융', '지주', '의', '지급', '보증', '한도', '8000', '억', '원', '또한', '잠재적', '부담', '으로', '작용', '하', '고', '있', '다', 'TRS', '잔액', '3400', '억', '원', '과', '보증', '한도', '8000', '억', '원', '은', '메리', '츠', '금융', '지주', '의', '별도', '기준', '자기', '자본', '1', '조', '7477', '억', '원', '의', '19', '5', '45', '8', '에', '해당', '하', 'ㄴ다', '특히나', '그룹', '전반', '의', '부동산', 'PF', '익스포저', '가', '과중', '하다', '고', '분석', '하', '었', '다', '2015', '년', '이후', '주력', '계열사', '의', 'PF', '대출', '이', '급증', '하', '어', '2021', '년', '12', '월', '말', '기준', '주력', '자회사', '가', '보유', '하', 'ㄴ', '부동산', 'PF', '순', '익스포저', '는', '17', '조', '3000', '억', '원', '이', '다', '이', '는', '연결', '자본', '대비', '374', '이', '며', '매입', '확약', '등', '을', '포함', '하', 'ㄴ', '부동산', '순', '익스', '포저', '합계', '는', '25', '조', '7000', '억', '원', '에', '달하', 'ㄴ다', '김', '연구원', '은', '향후', '부동산', '시장', '여건', '변화', '에', '따르', '아', '그룹', '차원', '의', '수익', '변동성', '과', '재무', '부담', '확대', '가능성', '이', '상당', '하', '다', '이', '고', '지적', '하', '었', '다', '윤', '재성', 'NICE', '신용', '평가', '연구원', '은', '상대적', '으로', '위험도', '가', '높', '은', '해외', '대체', '투자', '등', '고', '위험', '여신', '비중', '이', '높', '은', '수준', '이', '며', '메리', '츠', '화재', '역시', '대출', '채권', '의', '상당', '부분', '이', '부동산', 'PF', '대출', '로', '구성', '되', '어', '있', '는', '등', '그룹', '계열사', '전체적', '으로', '부동산', '관련', '여신', '집중도', '가', '높다', '며', '특히', '일부', '대체', '투자', '관련', '자산', '의', '경우', '동일', '차주', '에', '대하', 'ㄴ', '금융', '그룹', '의', '고액', '익스포저', '가', '존재', '하', '어', '특정', '차주', '부실', '화', '에', '따르', 'ㄴ', '동반', '부실', '위험도', '존재', '하', '는', '것', '으로', '판단', '되', 'ㄴ다', '고', '전하', '었', '다', '한편', '메리', '츠', '금융', '지주', '는', '이번', '에', '조달', '하', 'ㄴ', '자금', '가운데', '2000', '억', '원', '을', '회사채', '만기', '상환', '에', '사용', '하', 'ㄹ', '계획', '이', '다', '구체적', '으로', '오', '는', '26', '일', '만기', '가', '도래', '하', '는', '저', '의', '7', '회', '차', '사채', '다음', '달', '7', '일', '에', '만기', '가', '돌아오', '는', '저', '의', '10', '회', '차', '사채', '차환', '에', '사용', '하', 'ㄴ다', '메리', '츠', '금융', '지주', '는', '원자재', '값', '상승', '과', '추가', '적인', '기준', '금리', '인상', '으로', '발생', '하', 'ㄹ', '수', '있', '는', '유동성', '리스크', '등', '을', '고려', '하', '어', '나머지', '500', '억', '원', '을', '회사', '운영', '예비', '자금', '으로', '사용', '하', 'ㄹ', '예정', '이', '다'] 1\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "# 필드 지정 후 데이터셋 만들어야한다\n",
    "# 데이터를 불러오면서 필드에서 정의했던 방식으로 토큰화 수행하고 기본적인 전처리 진행해준다\n",
    "train, validation = TabularDataset.splits(\n",
    "    path = './',\n",
    "    train = 'train2.csv',\n",
    "    validation = 'validation2.csv',\n",
    "    format = 'csv',\n",
    "    fields = [('text', TEXT), ('label', LABEL)],\n",
    "    skip_header = True\n",
    ")\n",
    "# format = 데이터 형식\n",
    "# fields = Field 에서 지정한 것 불러와서 적용\n",
    "# skip_header = 데이터 첫 번째 줄 무시\n",
    "\n",
    "print(train[0].text, train[0].label)\n",
    "print(validation[0].text, validation[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터 개수와 차원 : torch.Size([74334, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "# torchtext.vocab.Vectors(name = 벡터를 포함하는 파일 이름,\n",
    "#                         cache = 캐시된 벡터의 디렉토리,\n",
    "#                         url = 캐시에서 벡터를 찾을 수 없는 경우 다운로드할 url,\n",
    "#                         )\n",
    "# \n",
    "\n",
    "# 임베딩 벡터 불러오기\n",
    "vectors = Vectors(name = './fasttext_newstoken')\n",
    "\n",
    "# 단어장 만들기\n",
    "TEXT.build_vocab(train,\n",
    "                 vectors = vectors, min_freq = 1, max_size = None)\n",
    "\n",
    "\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train, validation set 을 지정한 배치 사이즈만큼 로드하여 각각 batch data를 생성\n",
    "train_iter, validation_iter = BucketIterator.splits(\n",
    "    datasets = (train, validation),\n",
    "    batch_size = 8, # 각 batch 별 크기\n",
    "    device = device,\n",
    "    sort = False\n",
    ")\n",
    "\n",
    "print('임베딩 벡터 개수와 차원 : {}'.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textCNN 정의\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        # '''\n",
    "        # vocab_built : train 데이터로 생성한 단어장 인자로 받는다\n",
    "        # emb_dim : 임베딩 벡터의 dimension을 인자로 받는다 (100)\n",
    "        # dim_channel : feature map 이후로 생성되는 채널의 수를 인자로 받는다 (10)\n",
    "        # kernel_wins : filter의 크기를 리스트 형태로 받는다 ([3, 4, 5])\n",
    "        # nun_class : output 클래스 개수 (2)\n",
    "        # '''\n",
    "        \n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        # 임베딩 설정을 위해 vocab size * embedding dimension 크기의 행렬을 만든다\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "        # copy 복사해온다는거 같다\n",
    "        # fasttext 임베딩 벡터 값 가져오기\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        # input = 1, dim_channel = output, 나머지는 커널 사이즈\n",
    "        # conv2d 함수에 임베딩 결과를 전달해 fillter 생성\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # 오버피팅 방지\n",
    "        self.fc = nn.Linear(len(kernel_wins) * dim_channel, num_class)\n",
    "        # 클래스의 score 생성\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 순전파 정의\n",
    "        # output 계산 과정\n",
    "        \n",
    "        emb_x = self.embed(x)\n",
    "        # x 의 임베딩 정보 전달\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "        # 첫번째 축에 차원 추가\n",
    "        # [batch_size * 570854 * 100] -> [batch_size * 1 * 570854 * 100]\n",
    "        # 2차원 텍스트 데이터를 모델에 이미지처럼 넣기위해\n",
    "        \n",
    "        con_x = [self.relu(conv(emb_x)) for conv in self.convs]\n",
    "        \n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        # max polling\n",
    "        \n",
    "        fc_x = torch.cat(pool_x, dim = 1)\n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        \n",
    "        logit = self.fc(fc_x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "\n",
    "def train(model, device, train_itr, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0, 0\n",
    "    \n",
    "    for batch in train_itr:\n",
    "        \n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        # 연산을 위해 텍스트 데이터 역행렬로 변환\n",
    "        target.data.sub_(1)\n",
    "        # target 각 값을 1씩 줄인다\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        # gpu\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # loss gradient 할당 되는거 초기화\n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        # loss 누적시키기\n",
    "        result = torch.max(logit, 1)[1]\n",
    "        # 인덱스별로 계산된 logit 값에서 더 큰 확률 가진 클래스 저장\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "        \n",
    "    train_loss /= len(train_itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(train_itr.dataset)\n",
    "    \n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "\n",
    "def evaluate(model, device, itr):\n",
    "    \n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0, 0\n",
    "    \n",
    "    for batch in itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        target = batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit, 1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "        \n",
    "    test_loss /= len(itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(itr.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embed): Embedding(74334, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 10, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "train epoch : 1 \t Loss : 0.09122291428824275 \t Accuracy : 51.207794189453125%\n",
      "valid epoch : 1 \t Loss : 0.08657938344620518 \t Accuracy : 52.64460754394531%\n",
      "model saves at 52.64460754394531 accuracy\n",
      "--------------------\n",
      "train epoch : 2 \t Loss : 0.08686686824614834 \t Accuracy : 52.100341796875%\n",
      "valid epoch : 2 \t Loss : 0.08651517183831801 \t Accuracy : 52.595333099365234%\n",
      "--------------------\n",
      "train epoch : 3 \t Loss : 0.08666141512105724 \t Accuracy : 52.72187805175781%\n",
      "valid epoch : 3 \t Loss : 0.08563440170611958 \t Accuracy : 54.291954040527344%\n",
      "model saves at 54.291954040527344 accuracy\n",
      "--------------------\n",
      "train epoch : 4 \t Loss : 0.08592686023081501 \t Accuracy : 54.14860916137695%\n",
      "valid epoch : 4 \t Loss : 0.08374254720810045 \t Accuracy : 56.58323669433594%\n",
      "model saves at 56.58323669433594 accuracy\n",
      "--------------------\n",
      "train epoch : 5 \t Loss : 0.08459704489548814 \t Accuracy : 55.717567443847656%\n",
      "valid epoch : 5 \t Loss : 0.08177594505323461 \t Accuracy : 57.9226188659668%\n",
      "model saves at 57.9226188659668 accuracy\n",
      "--------------------\n",
      "train epoch : 6 \t Loss : 0.08299433486519979 \t Accuracy : 57.078224182128906%\n",
      "valid epoch : 6 \t Loss : 0.07970304426974563 \t Accuracy : 59.99888229370117%\n",
      "model saves at 59.99888229370117 accuracy\n",
      "--------------------\n",
      "train epoch : 7 \t Loss : 0.08144427493429042 \t Accuracy : 58.32353591918945%\n",
      "valid epoch : 7 \t Loss : 0.07852933995508722 \t Accuracy : 61.191558837890625%\n",
      "model saves at 61.191558837890625 accuracy\n",
      "--------------------\n",
      "train epoch : 8 \t Loss : 0.08004356919717893 \t Accuracy : 59.77938461303711%\n",
      "valid epoch : 8 \t Loss : 0.07726495297074658 \t Accuracy : 62.711238861083984%\n",
      "model saves at 62.711238861083984 accuracy\n",
      "--------------------\n",
      "train epoch : 9 \t Loss : 0.07860236972648851 \t Accuracy : 61.13108444213867%\n",
      "valid epoch : 9 \t Loss : 0.07486490390548595 \t Accuracy : 64.48737335205078%\n",
      "model saves at 64.48737335205078 accuracy\n",
      "--------------------\n",
      "train epoch : 10 \t Loss : 0.07736127868422375 \t Accuracy : 62.1356201171875%\n",
      "valid epoch : 10 \t Loss : 0.07285504216093908 \t Accuracy : 65.79988098144531%\n",
      "model saves at 65.79988098144531 accuracy\n",
      "--------------------\n",
      "train epoch : 11 \t Loss : 0.07600314259559152 \t Accuracy : 63.21182632446289%\n",
      "valid epoch : 11 \t Loss : 0.07137963566243868 \t Accuracy : 65.70244598388672%\n",
      "--------------------\n",
      "train epoch : 12 \t Loss : 0.07503003272859932 \t Accuracy : 63.935272216796875%\n",
      "valid epoch : 12 \t Loss : 0.07011788941872243 \t Accuracy : 67.72943878173828%\n",
      "model saves at 67.72943878173828 accuracy\n",
      "--------------------\n",
      "train epoch : 13 \t Loss : 0.07376303101975677 \t Accuracy : 65.16714477539062%\n",
      "valid epoch : 13 \t Loss : 0.06904692144379136 \t Accuracy : 67.86382293701172%\n",
      "model saves at 67.86382293701172 accuracy\n",
      "--------------------\n",
      "train epoch : 14 \t Loss : 0.07287623713932857 \t Accuracy : 65.94770050048828%\n",
      "valid epoch : 14 \t Loss : 0.06747757608046043 \t Accuracy : 68.84596252441406%\n",
      "model saves at 68.84596252441406 accuracy\n",
      "--------------------\n",
      "train epoch : 15 \t Loss : 0.07188387532192873 \t Accuracy : 66.79881286621094%\n",
      "valid epoch : 15 \t Loss : 0.06607519076249856 \t Accuracy : 69.90201568603516%\n",
      "model saves at 69.90201568603516 accuracy\n",
      "--------------------\n",
      "train epoch : 16 \t Loss : 0.07100230371026231 \t Accuracy : 67.48978424072266%\n",
      "valid epoch : 16 \t Loss : 0.06607205923745771 \t Accuracy : 70.84159088134766%\n",
      "model saves at 70.84159088134766 accuracy\n",
      "--------------------\n",
      "train epoch : 17 \t Loss : 0.07017353354551216 \t Accuracy : 68.09676361083984%\n",
      "valid epoch : 17 \t Loss : 0.06483941084041285 \t Accuracy : 71.59527587890625%\n",
      "model saves at 71.59527587890625 accuracy\n",
      "--------------------\n",
      "train epoch : 18 \t Loss : 0.0691236122862757 \t Accuracy : 68.88851928710938%\n",
      "valid epoch : 18 \t Loss : 0.06410397744605473 \t Accuracy : 72.2481689453125%\n",
      "model saves at 72.2481689453125 accuracy\n",
      "--------------------\n",
      "train epoch : 19 \t Loss : 0.06859346818644267 \t Accuracy : 69.45069885253906%\n",
      "valid epoch : 19 \t Loss : 0.06324589037590321 \t Accuracy : 72.86186218261719%\n",
      "model saves at 72.86186218261719 accuracy\n",
      "--------------------\n",
      "train epoch : 20 \t Loss : 0.06781880672180651 \t Accuracy : 69.8113021850586%\n",
      "valid epoch : 20 \t Loss : 0.061594531395814105 \t Accuracy : 72.56173706054688%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 성능 확인\n",
    "\n",
    "model = TextCNN(vocab, 100, 10, [3, 4, 5], 2).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "best_test_acc = -1\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer)\n",
    "    \n",
    "    print('train epoch : {} \\t Loss : {} \\t Accuracy : {}%'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, device, validation_iter)\n",
    "    \n",
    "    print('valid epoch : {} \\t Loss : {} \\t Accuracy : {}%'.format(epoch, val_loss, val_acc))\n",
    "    \n",
    "    if val_acc > best_test_acc:\n",
    "        best_test_acc = val_acc\n",
    "        \n",
    "        print('model saves at {} accuracy'.format(best_test_acc))\n",
    "        torch.save(model.state_dict(), 'TextCNN_Best_Validation.pt')\n",
    "        \n",
    "    print('-' * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5032d577b80ebef2f7ce9eacaf1606914e2f8bc4f54bb832533ded3e41133f5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
